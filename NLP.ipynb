{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of proetry on the basis of different genre \n",
    "In this project the poetry has been classified into different geners using two model namely\n",
    "1) Naive bayes classifier\n",
    "2) Logistic Regresson for classification\n",
    "\n",
    "The data set has been taken form Kaggle (https://www.kaggle.com/ultrajack/modern-renaissance-poetry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     author                                      content  \\\n",
      "count                   573                                          573   \n",
      "unique                   67                                          506   \n",
      "top     WILLIAM SHAKESPEARE  Originally published in Poetry, March 1914.   \n",
      "freq                     71                                            4   \n",
      "\n",
      "       poem name          age  type  \n",
      "count        571          573   573  \n",
      "unique       508            2     3  \n",
      "top     Canto IV  Renaissance  Love  \n",
      "freq           3          315   326  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv(\"C:/Users/Nirvan Dogra/Desktop/poems-from-poetryfoundation-org/all.csv\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(dataset.describe())\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['content'], dataset['type'] , test_size=0.2, random_state=42) #splitting the data into test and training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Data\n",
    "-> The data set contains 573 data values\n",
    "-> It has 5 colums namely (author, content, poem name, age, type)\n",
    "-> The most common vaue is William Shakespeare containg a totat of 71 data values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-pocessing of data\n",
    "For the pre-porcessing of data a library named nlkt has been used.For the pre-processing of data the following steps have been take:\n",
    "1) Conversion to lower case\n",
    "2) Bag of words\n",
    "3) Removal of stop words\n",
    "4) Removal of puntuation\n",
    "5) Lemmatizer\n",
    "6) conversion of text to sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def text_process(example_sent):\n",
    "    example_sent = example_sent.lower()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    word_tokens = word_tokenize(example_sent) \n",
    "    \n",
    "    punctuation=['.',',','!',';','(',')','-', '_']\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens:\n",
    "        if not (w in stop_words) and not (w in punctuation):\n",
    "            filtered_sentence.append(w)\n",
    "    return(filtered_sentence) \n",
    "        \n",
    "  \n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     base_form=[];\n",
    "#     for w in filtered_sentence:\n",
    "#         print(w,\" \", lemmatizer.lemmatize(w, pos='a'))\n",
    "#         base_form.append(lemmatizer.lemmatize(w, pos='a'))\n",
    "#     print(base_form);\n",
    "\n",
    "# example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "# for example_sent in X_train:\n",
    "    \n",
    "#     example_sent = example_sent.lower()\n",
    "#     stop_words = set(stopwords.words('english')) \n",
    "\n",
    "#     word_tokens = word_tokenize(example_sent) \n",
    "#     print(word_tokens)\n",
    "#     punctuation=['.',',','!',';','(',')','-', '_']\n",
    "#     filtered_sentence = [] \n",
    "#     for w in word_tokens:\n",
    "#         if not (w in stop_words) and not (w in punctuation):\n",
    "#             filtered_sentence.append(w)\n",
    "#     print(filtered_sentence)  \n",
    "        \n",
    "  \n",
    "#     lemmatizer = WordNetLemmatizer() \n",
    "#     base_form=[];\n",
    "#     for w in filtered_sentence:\n",
    "#         print(w,\" \", lemmatizer.lemmatize(w, pos='a'))\n",
    "#         base_form.append(lemmatizer.lemmatize(w, pos='a'))\n",
    "#     print(base_form);\n",
    "    \n",
    "#     word2count={}\n",
    "#     for w in base_form: \n",
    "#         if w not in word2count.keys(): \n",
    "#             word2count[w] = 1\n",
    "#         else: \n",
    "#             word2count[w] += 1\n",
    "            \n",
    "#         print(word2count);\n",
    "        \n",
    "        \n",
    "#                 # Importing necessary libraries\n",
    "        \n",
    "#         # instantiating the model with Multinomial Naive Bayes..\n",
    "#         model = MultinomialNB()\n",
    "#         # training the model...\n",
    "#         model = model.fit(base_form, y_train)\n",
    "\n",
    "# filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "# \n",
    "  \n",
    "# for w in word_tokens: \n",
    "#     if w not in stop_words: \n",
    "#         filtered_sentence.append(w) \n",
    "  \n",
    "# print(word_tokens) \n",
    "# print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bow_transformer=CountVectorizer(analyzer=text_process).fit(X_train)\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_train=bow_transformer.transform(X_train)\n",
    "# transforming into Bag-of-Words and hence textual data to numeric..\n",
    "text_bow_test=bow_transformer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Naive Bayes # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# instantiating the model with Multinomial Naive Bayes..\n",
    "model = MultinomialNB()\n",
    "# training the model...\n",
    "model = model.fit(text_bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868995633187773"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(text_bow_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                Love       0.72      0.87      0.79        68\n",
      "Mythology & Folklore       0.20      0.11      0.14         9\n",
      "              Nature       0.68      0.50      0.58        38\n",
      "\n",
      "           micro avg       0.69      0.69      0.69       115\n",
      "           macro avg       0.53      0.49      0.50       115\n",
      "        weighted avg       0.67      0.69      0.67       115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.metrics import classification_report\n",
    " \n",
    "# getting the predictions of the Validation Set...\n",
    "predictions = model.predict(text_bow_test)\n",
    "# getting the Precision, Recall, F1-Score\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring form the results\n",
    "The model has an accuracy of 86.89%\n",
    "The model works best at classifying 'Love' and worst for the classification of 'Mythology & Folklore'. To compensate for this, we can use weighted metrics on the Mythology and Folklore classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Logistic regression # # #\n",
    "The preprocessing of the data remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "lm=LogisticRegression(solver='lbfgs', multi_class='multinomial').fit(text_bow_train, y_train)\n",
    "predicted_classes = lm.predict(text_bow_test)\n",
    "#print(predicted_classes)\n",
    "accuracy = accuracy_score(y_test,predicted_classes)\n",
    "parameters = model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of non convergence:\n",
    "\n",
    "The model doesnt converge due to the presence of mulitple local minima, each sub-localized to the genre. Since there are multiple genre to be covered, it only makes sense that the minima is different for each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6782608695652174\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inferring from the results\n",
    "The accuracy of the model is 67.82%\n",
    "This model preforms worse than the previous one.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comments\n",
    "\n",
    "The accuracy of the above model can be improved with the use of explicit minima, i.e. using different minima in training models for each genre.\n",
    "The accuracy of the earlier model can be improved with a change in either pruning via AdaBoost, or by using weighted metrics and post pruning the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
